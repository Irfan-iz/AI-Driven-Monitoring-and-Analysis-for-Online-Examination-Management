{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162ed64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful!\n",
      "Loading BERT model... (This will download ~420MB the first time)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c04accd8174a77bedbb5c24c938399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User1\\OneDrive\\Desktop\\workshop 2\\llm_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User1\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e174c0696e0441eab5b91d5c4785a387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2186c0c30a684602a4dd2947753f8337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7840aa8a494407da18d621c4d5d0343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084828778bc64de69430374150e51171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Success! BERT model and tokenizer are loaded into memory.\n",
      "Your AI development environment is ready!\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Import the tools ---\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(\"Import successful!\")\n",
    "\n",
    "# --- 2. Load the pre-trained BERT model ---\n",
    "print(\"Loading BERT model... (This will download ~420MB the first time)...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Success! BERT model and tokenizer are loaded into memory.\")\n",
    "print(\"Your AI development environment is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a6103a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function 'get_sentence_embedding' is defined.\n",
      "This function will now turn any sentence into a 768-dimension vector.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: The Embedding Function ---\n",
    "# We need a function to turn a sentence into a single vector (embedding)\n",
    "import torch # We need this to handle the model's output\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    # 1. Tokenize: Turn the sentence into numbers (token IDs)\n",
    "    #    return_tensors='pt' tells it to return PyTorch tensors\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "    \n",
    "    # 2. Get Embeddings: Pass the tokens through the loaded BERT model\n",
    "    #    We wrap this in 'torch.no_grad()' for efficiency (we're not training, just inferring)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 3. Get the [CLS] Token:\n",
    "    #    The 'last_hidden_state' has the vector for every token.\n",
    "    #    The very first token is the [CLS] token, which is trained to \n",
    "    #    represent the meaning of the whole sentence.\n",
    "    #    'outputs.last_hidden_state' is [batch_size, num_tokens, hidden_size]\n",
    "    #    We take [0, 0, :] to get the [CLS] token of the first (and only) sentence.\n",
    "    cls_embedding = outputs.last_hidden_state[0, 0, :]\n",
    "    \n",
    "    return cls_embedding\n",
    "\n",
    "print(\"Helper function 'get_sentence_embedding' is defined.\")\n",
    "print(\"This function will now turn any sentence into a 768-dimension vector.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "012ab139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for all sentences...\n",
      "Calculating similarity...\n",
      "\n",
      "--- Results ---\n",
      "Similarity (Paraphrased): 0.9370\n",
      "Similarity (Different Topics): 0.8092\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Run the Similarity Test ---\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np # We need numpy to reshape the vectors for the similarity function\n",
    "\n",
    "# --- Our Test Sentences ---\n",
    "# Case 1: Paraphrased (Should be HIGH similarity)\n",
    "text_a = \"The AI-Driven Intelligent Exam Integrity System is a project that helps to make exams more fair.\"\n",
    "text_b = \"A project called the AI-Driven Intelligent Exam Integrity System makes exams more honest.\"\n",
    "\n",
    "# Case 2: Different Topics (Should be LOW similarity)\n",
    "text_c = \"The student submitted the final report on Tuesday.\"\n",
    "text_d = \"The cat slept on the warm keyboard.\"\n",
    "\n",
    "# --- Generate Embeddings ---\n",
    "print(\"Generating embeddings for all sentences...\")\n",
    "emb_a = get_sentence_embedding(text_a)\n",
    "emb_b = get_sentence_embedding(text_b)\n",
    "emb_c = get_sentence_embedding(text_c)\n",
    "emb_d = get_sentence_embedding(text_d)\n",
    "\n",
    "# --- Calculate Similarity ---\n",
    "# Cosine similarity expects 2D arrays, so we use .reshape(1, -1) to format them\n",
    "print(\"Calculating similarity...\")\n",
    "\n",
    "# Test Case 1\n",
    "sim_paraphrased = cosine_similarity(emb_a.reshape(1, -1), emb_b.reshape(1, -1))[0][0]\n",
    "\n",
    "# Test Case 2\n",
    "sim_different = cosine_similarity(emb_c.reshape(1, -1), emb_d.reshape(1, -1))[0][0]\n",
    "\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Similarity (Paraphrased): {sim_paraphrased:.4f}\")\n",
    "print(f\"Similarity (Different Topics): {sim_different:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca7a1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from sentence-transformers) (2.9.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Downloading pillow-12.0.0-cp313-cp313-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user1\\onedrive\\desktop\\workshop 2\\llm_env\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading pillow-12.0.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/7.0 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.8/7.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.1/7.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.2/7.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.5/7.0 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.8/7.0 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 5.7 MB/s  0:00:01\n",
      "Installing collected packages: Pillow, sentence-transformers\n",
      "\n",
      "   ---------------------------------------- 0/2 [Pillow]\n",
      "   ---------------------------------------- 0/2 [Pillow]\n",
      "   ---------------------------------------- 0/2 [Pillow]\n",
      "   ---------------------------------------- 0/2 [Pillow]\n",
      "   ---------------------------------------- 0/2 [Pillow]\n",
      "   ---------------------------------------- 0/2 [Pillow]\n",
      "   ---------------------------------------- 0/2 [Pillow]\n",
      "   ---------------------------------------- 0/2 [Pillow]\n",
      "   ---------------------------------------- 0/2 [Pillow]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   -------------------- ------------------- 1/2 [sentence-transformers]\n",
      "   ---------------------------------------- 2/2 [sentence-transformers]\n",
      "\n",
      "Successfully installed Pillow-12.0.0 sentence-transformers-5.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Sentence-Transformers library installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 4: Install the SBERT library ---\n",
    "%pip install -U sentence-transformers\n",
    "\n",
    "print(\"Sentence-Transformers library installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b12c40cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading specialized SBERT model (all-MiniLM-L6-v2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9f28353be64312ac9875bd79b3a14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User1\\OneDrive\\Desktop\\workshop 2\\llm_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User1\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73dcffba0c94e0aa0c53df5eb3f3162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9dbf83fd394b229d62428b00159c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d837eda35b41ce917318864c6b5082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98877ce1e0ad419a85480dec860ce11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f528afee8764fd7bd92723bd0f9a67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646022d1bf534955948c0ef8203c62fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b636f3f14b3d452cae186d089a2d4544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9373f92177124adbb1263c912c8b863b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ecf3c56af4419fad4eabc32b7da4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da69c0636c5d4de89dc8af061a848c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results from Specialized SBERT Model ---\n",
      "Similarity (Paraphrased): 0.9421\n",
      "Similarity (Different Topics): 0.0760\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5: Re-run test with a specialized SBERT model ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. Load the specialized SBERT model\n",
    "#    This will download the new model (~80MB)\n",
    "print(\"Loading specialized SBERT model (all-MiniLM-L6-v2)...\")\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# --- Our Test Sentences (Same as before) ---\n",
    "# Case 1: Paraphrased\n",
    "text_a = \"The AI-Driven Intelligent Exam Integrity System is a project that helps to make exams more fair.\"\n",
    "text_b = \"A project called the AI-Driven Intelligent Exam Integrity System makes exams more honest.\"\n",
    "\n",
    "# Case 2: Different Topics\n",
    "text_c = \"The student submitted the final report on Tuesday.\"\n",
    "text_d = \"The cat slept on the warm keyboard.\"\n",
    "\n",
    "# --- 2. Generate Embeddings (Simpler!) ---\n",
    "# The new model encodes a list of sentences all at once\n",
    "embeddings = sbert_model.encode([text_a, text_b, text_c, text_d])\n",
    "\n",
    "# --- 3. Calculate Similarity ---\n",
    "sim_paraphrased = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "sim_different = cosine_similarity([embeddings[2]], [embeddings[3]])[0][0]\n",
    "\n",
    "print(\"\\n--- Results from Specialized SBERT Model ---\")\n",
    "print(f\"Similarity (Paraphrased): {sim_paraphrased:.4f}\")\n",
    "print(f\"Similarity (Different Topics): {sim_different:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaed4e2",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29c68cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is loaded.\n",
      "\n",
      "--- Plagiarism Check Results ---\n",
      "Student Answer: 'This AI-driven intelligent system will help make exams more fair and honest.'\n",
      "Similarity Scores (vs Sources 0, 1, 2): [0.00295392 0.86295974 0.16966324]\n",
      "---\n",
      "Highest Score: 0.8630\n",
      "Source Document (Index 1): 'An AI-driven intelligent system is a project that helps to make exams more fair.'\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6: Find Best Match from a \"Database\" ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load the Model (if in a new session) ---\n",
    "# If you just ran Cell 5, the model is already in memory\n",
    "# If not, uncomment the line below:\n",
    "# sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model is loaded.\")\n",
    "\n",
    "\n",
    "# --- 2. Define our Data ---\n",
    "\n",
    "# This is the new submission we're checking\n",
    "student_answer = \"This AI-driven intelligent system will help make exams more fair and honest.\"\n",
    "\n",
    "# This is our \"database\" of known sources\n",
    "source_database = [\n",
    "    \"The cat slept on the warm keyboard.\", # Source 0 (Different)\n",
    "    \"An AI-driven intelligent system is a project that helps to make exams more fair.\", # Source 1 (Paraphrased)\n",
    "    \"The student submitted the final report on Tuesday.\" # Source 2 (Different)\n",
    "]\n",
    "\n",
    "# --- 3. Encode ALL texts ---\n",
    "# Encode the student answer\n",
    "student_embedding = sbert_model.encode(student_answer)\n",
    "\n",
    "# Encode all the documents in our database\n",
    "source_embeddings = sbert_model.encode(source_database)\n",
    "\n",
    "\n",
    "# --- 4. Calculate Similarity ---\n",
    "# We compare the single student embedding against ALL the source embeddings\n",
    "# This returns a list of scores, e.g., [[score_vs_0, score_vs_1, score_vs_2]]\n",
    "similarity_scores = cosine_similarity(\n",
    "    [student_embedding],  # Needs to be in a list (2D array)\n",
    "    source_embeddings     # Is already a list of embeddings (2D array)\n",
    ")[0] # Get the first (and only) row of scores\n",
    "\n",
    "# --- 5. Find the Best Match ---\n",
    "best_match_index = np.argmax(similarity_scores)\n",
    "best_match_score = similarity_scores[best_match_index]\n",
    "best_match_source = source_database[best_match_index]\n",
    "\n",
    "print(\"\\n--- Plagiarism Check Results ---\")\n",
    "print(f\"Student Answer: '{student_answer}'\")\n",
    "print(f\"Similarity Scores (vs Sources 0, 1, 2): {similarity_scores}\")\n",
    "print(\"---\")\n",
    "print(f\"Highest Score: {best_match_score:.4f}\")\n",
    "print(f\"Source Document (Index {best_match_index}): '{best_match_source}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
